{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch `nn` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**from last lecture**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn`\n",
    "\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into layers, some of which \n",
    "have learnable parameters which will be optimized during learning.\n",
    "\n",
    "In TensorFlow, packages like **Keras**, (old **TensorFlow-Slim**, and **TFLearn**) provide higher-level abstractions over raw computational graphs that are useful for building neural networks.\n",
    "\n",
    "In PyTorch, the `nn` package serves this same purpose. \n",
    "\n",
    "The `nn` package defines a set of `Module`s, which are roughly equivalent to neural network layers. \n",
    "\n",
    "A `Module` receives input `Tensor`s and computes output `Tensor`s, but may also hold internal state such as `Tensor`s containing learnable parameters. \n",
    "\n",
    "The `nn` package also defines a set of useful `loss` functions that are commonly used when \n",
    "training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use the `nn` package to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 639.6167602539062\n",
      "50 30.772613525390625\n",
      "100 2.5433220863342285\n",
      "150 0.4780966639518738\n",
      "200 0.12993700802326202\n",
      "250 0.04030954837799072\n",
      "300 0.013411465100944042\n",
      "350 0.004547220654785633\n",
      "400 0.0015493093524128199\n",
      "450 0.000528861943166703\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.optim`\n",
    "\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters (**using `torch.no_grad()` or `.data` to avoid tracking history in autograd**). \n",
    "\n",
    "This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like `AdaGrad`, `RMSProp`, \n",
    "`Adam`.\n",
    "\n",
    "The optim package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "Let's finally modify the previous example in order to use `torch.optim` and the `Adam` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 668.5765380859375\n",
      "50 201.50619506835938\n",
      "100 49.221805572509766\n",
      "150 6.844631195068359\n",
      "200 0.6239314079284668\n",
      "250 0.0545228011906147\n",
      "300 0.005567540414631367\n",
      "350 0.0005590067594312131\n",
      "400 4.745638943859376e-05\n",
      "450 3.2170055419555865e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model and Optimiser (w/ Parameters) at a glance\n",
    "\n",
    "![model_and_optimiser](model_optim.png)\n",
    "\n",
    "<span class=\"fn\"><i>Source:</i> [1] - _Deep Learning with PyTorch_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Learning Process\n",
    "\n",
    "![learning process sketch](./learning_process.png)\n",
    "\n",
    "<span class=\"fn\"><i>Source:</i> [1] - _Deep Learning with PyTorch_ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible scenarios:\n",
    "\n",
    "- Specify models that are more complex than a sequence of existing (pre-defined) modules;\n",
    "- Customise the learning procedure (e.g. _weight sharing_ ?)\n",
    "- ?\n",
    "\n",
    "For these cases, **PyTorch** allows to define our own custom modules by subclassing `nn.Module` and defining a `forward` method which receives the input data (i.e. `Tensor`) and returns the output (i.e. `Tensor`).\n",
    "\n",
    "It is in the `forward` method that **all** the _magic_ of Dynamic Graph and `autograd` operations happen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Custom Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's implement our **two-layers** model as a custom `nn.Module` subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.hidden_activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        l1 = self.linear1(x)\n",
    "        h_relu = self.hidden_activation(l1)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 693.8279418945312\n",
      "50 33.77482986450195\n",
      "100 2.100647449493408\n",
      "150 0.24654023349285126\n",
      "200 0.04137988016009331\n",
      "250 0.008377175778150558\n",
      "300 0.0019239461980760098\n",
      "350 0.0004813372506760061\n",
      "400 0.00012718062498606741\n",
      "450 3.48151006619446e-05\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened really? Let's have a closer look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> model = TwoLayerNet(D_in, H, D_out)\n",
    "```\n",
    "\n",
    "This calls `TwoLayerNet.__init__` **constructor** method (_implementation reported below_ ):\n",
    "\n",
    "```python\n",
    "def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.hidden_activation = torch.nn.ReLU()\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "```\n",
    "\n",
    "1. First thing, we call the `nn.Module` constructor which sets up the housekeeping\n",
    "    - If you forget to do that, you will get and error message reminding that you should call it before using any `nn.Module` capabilities\n",
    "2. We create a class attribute for each layer (`OP/Tensor/`) that we intend to include in our model\n",
    "    - These can be also `Sequential` as in _Submodules_ or *Block of Layers*\n",
    "    - **Note**: We are **not** defining the Graph yet, just the layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> y_pred = model(x)\n",
    "```\n",
    "\n",
    "1. First thing to notice: the `model` object is **callable**\n",
    "   - It means `nn.Module` is implementing a `__call__` method\n",
    "   - We **don't** need to re-implement that!\n",
    "   \n",
    "2. (in fact) The `nn.Module` class will call `self.forward` - in a [Template Method Pattern](https://en.wikipedia.org/wiki/Template_method_pattern) fashion\n",
    "    - for this reason, we have to define the `forward` method\n",
    "    - (needless to say) the `forward` method implements the **forward** pass of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from torch.nn.modules.module.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "class Module(object):\n",
    "    # [...] omissis\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        for hook in self._forward_pre_hooks.values():\n",
    "            result = hook(self, input)\n",
    "            if result is not None:\n",
    "                if not isinstance(result, tuple):\n",
    "                    result = (result,)\n",
    "                input = result\n",
    "        if torch._C._get_tracing_state():\n",
    "            result = self._slow_forward(*input, **kwargs)\n",
    "        else:\n",
    "            result = self.forward(*input, **kwargs)\n",
    "        for hook in self._forward_hooks.values():\n",
    "            hook_result = hook(self, input, result)\n",
    "            if hook_result is not None:\n",
    "                result = hook_result\n",
    "        if len(self._backward_hooks) > 0:\n",
    "            var = result\n",
    "            while not isinstance(var, torch.Tensor):\n",
    "                if isinstance(var, dict):\n",
    "                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "                else:\n",
    "                    var = var[0]\n",
    "            grad_fn = var.grad_fn\n",
    "            if grad_fn is not None:\n",
    "                for hook in self._backward_hooks.values():\n",
    "                    wrapper = functools.partial(hook, self)\n",
    "                    functools.update_wrapper(wrapper, hook)\n",
    "                    grad_fn.register_hook(wrapper)\n",
    "        return result\n",
    "    \n",
    "    # [...] omissis\n",
    "    def forward(self, *input):\n",
    "        r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Should be overridden by all subclasses.\n",
    "\n",
    "        .. note::\n",
    "            Although the recipe for forward pass needs to be defined within\n",
    "            this function, one should call the :class:`Module` instance afterwards\n",
    "            instead of this since the former takes care of running the\n",
    "            registered hooks while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take away messages** :\n",
    "1. We don't need to implement the `__call__` method at all in our custom model subclass\n",
    "2. We don't need to call the `forward` method directly. \n",
    "    - We could, but we would miss the flexibility of _forward_ and _backwar_ hooks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Last but not least\n",
    "\n",
    "```python\n",
    ">>> optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "```\n",
    "\n",
    "Being `model` a subclass of `nn.Module`, `model.parameters()` will automatically capture all the `Layers/OP/Tensors/Parameters` that require gradient computation, so to feed to the `autograd` engine during the *backward* (optimisation) step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `model.named_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight        torch.Size([100, 1000]) 100000\n",
      "linear1.bias          torch.Size([100])   100\n",
      "linear2.weight        torch.Size([10, 100]) 1000\n",
      "linear2.bias          torch.Size([10])    10\n"
     ]
    }
   ],
   "source": [
    "for name_str, param in model.named_parameters():\n",
    "    print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAIT**: What happened to `hidden_activation` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.hidden_activation = torch.nn.ReLU()\n",
    "```\n",
    "\n",
    "So, it looks that we are registering in the constructor a submodule (`torch.nn.ReLU`) that has no parameters.\n",
    "\n",
    "Generalising, if we would've had **more** (hidden) layers, it would have required the definition of one of these submodules for each pair of layers (at least)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back at the implementation of the `TwoLayerNet` class as a whole, it looks like a bit of a waste.\n",
    "\n",
    "**Can we do any better here?** 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, in this particular case, we could implement the `ReLU` activation _manually_, it is not that difficult, isn't it?\n",
    "\n",
    "$\\rightarrow$ As we already did before, we could use the [`torch.clamp`](https://pytorch.org/docs/stable/torch.html?highlight=clamp#torch.clamp) function\n",
    "\n",
    "> `torch.clamp`: Clamp all elements in input into the range [ min, max ] and return a resulting tensor\n",
    "\n",
    "`t.clamp(min=0)` is **exactly** the ReLU that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sorted!\n",
    "\n",
    "That was easy, wasn't it? **However**, what if we wanted *other* activation functions (e.g. `tanh`, \n",
    "`sigmoid`, `LeakyReLU`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Functional API\n",
    "\n",
    "PyTorch has functional counterparts of every `nn` module. \n",
    "\n",
    "By _functional_ here we mean \"having no internal state\", or, in other words, \"whose output value is solely and fully determined by the value input arguments\". \n",
    "\n",
    "Indeed, `torch.nn.functional` provides the many of the same modules we find in `nn`, but with all eventual parameters moved as an argument to the function call. \n",
    "\n",
    "For instance, the functional counterpart of `nn.Linear` is `nn.functional.linear`, which is a function that has signature `linear(input, weight, bias=None)`. \n",
    "\n",
    "The `weight` and `bias` parameters are **arguments** to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our `TwoLayerNet` model, it makes sense to keep using nn modules for `nn.Linear`, so that our model will be able to manage all of its `Parameter` instances during training. \n",
    "\n",
    "However, we can safely switch to the functional counterparts of `nn.ReLU`, since it has no parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = torch.nn.functional.relu(self.linear1(x))  # torch.relu would do as well\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "F.relu(self.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 691.5662841796875\n",
      "50 38.10184097290039\n",
      "100 2.986912727355957\n",
      "150 0.27371305227279663\n",
      "200 0.0322413295507431\n",
      "250 0.00451896945014596\n",
      "300 0.000707552710082382\n",
      "350 0.00011905599967576563\n",
      "400 2.109917659254279e-05\n",
      "450 3.876339633279713e-06\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ For the curious minds: [The difference and connection between torch.nn and torch.nn.function from relu's various implementations](https://programmer.group/5d5a404b257d7.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clever advice and Rule of thumb\n",
    "\n",
    "> With **quantization**, stateless bits like activations suddenly become stateful because information on the quantization needs to be captured. This means that if we aim to quantize our model, it might be worthwile to stick with the modular API if we go for non-JITed quantization. There is one style matter that will help you avoid surprises with (originally unforseen) uses: if you need several applications of stateless modules (like `nn.HardTanh` or `nn.ReLU`), it is likely a good idea to have a separate instance for each. Re-using the same module appears to be clever and will give correct results with our standard Python usage here, but tools analysing your model might trip over it.\n",
    "\n",
    "<span class=\"fn\"><i>Source:</i> [1] - _Deep Learning with PyTorch_ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Graph flow: Example of Weight Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already discussed, the definition of custom `nn.Module` in PyTorch requires the definition of layers (i.e. Parameters) in the constructor (`__init__`), and the implementation of the `forward` method in which the dynamic graph will be traversed defined by the call to each of those layers/parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of **dynamic graphs** we are going to implement a scenario in which we require parameters (i.e. _weights_) sharing between layers.\n",
    "\n",
    "In order to do so, we will implement a very odd model: a fully-connected ReLU network that on each `forward` call chooses a `random` number (between 1 and 4) and uses that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do so, we will implement _weight sharing_ among the innermost layers by simply reusing the same `Module` multiple times when defining the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = torch.relu(self.input_linear(x))\n",
    "        hidden_layers = random.randint(0, 3)\n",
    "        for _ in range(hidden_layers):\n",
    "            h_relu = torch.relu(self.middle_linear(h_relu))\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 319.9242858886719\n",
      "0 0.0\n",
      "50 0.0\n",
      "50 0.0\n",
      "100 0.0\n",
      "100 0.0\n",
      "150 0.0\n",
      "150 0.0\n",
      "200 0.0\n",
      "200 0.0\n",
      "250 0.0\n",
      "250 0.0\n",
      "300 0.0\n",
      "300 0.0\n",
      "350 0.0\n",
      "350 0.0\n",
      "400 0.0\n",
      "400 0.0\n",
      "450 0.0\n",
      "450 0.0\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    for i in range(2):\n",
    "        start, end = int((N/2)*i), int((N/2)*(i+1))\n",
    "        x = x[start:end, ...]\n",
    "        y = y[start:end, ...]\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        if t % 50 == 0:\n",
    "            print(t, loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latest from the `torch` ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\rightarrow$: [Migration from Chainer to PyTorch](https://medium.com/pytorch/migration-from-chainer-to-pytorch-8ed92c12c8)\n",
    "\n",
    "* $\\rightarrow$: [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html)\n",
    "    - [fast.ai](https://docs.fast.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### References and Futher Reading:\n",
    "\n",
    "1. [Deep Learning with PyTorch, Luca Antiga et. al.](https://www.manning.com/books/deep-learning-with-pytorch)\n",
    "2. [(**Terrific**) PyTorch Examples Repo](https://github.com/jcjohnson/pytorch-examples) (*where most of the examples in this notebook have been adapted from*)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL-TORCH)",
   "language": "python",
   "name": "dl-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
